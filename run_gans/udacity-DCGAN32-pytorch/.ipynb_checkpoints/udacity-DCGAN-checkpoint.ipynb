{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Generation\n",
    "\n",
    "In this project, you'll define and train a DCGAN on a dataset of faces. Your goal is to get a generator network to generate *new* images of faces that look as realistic as possible!\n",
    "\n",
    "The project will be broken down into a series of tasks from **loading in data to defining and training adversarial networks**. At the end of the notebook, you'll be able to visualize the results of your trained Generator to see how it performs; your generated samples should look like fairly realistic faces with small amounts of noise.\n",
    "\n",
    "### Get the Data\n",
    "\n",
    "You'll be using the [CelebFaces Attributes Dataset (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) to train your adversarial networks.\n",
    "\n",
    "This dataset is more complex than the number datasets (like MNIST or SVHN) you've been working with, and so, you should prepare to define deeper networks and train them for a longer time to get good results. <u><b>It is suggested that you utilize a GPU for training.</b></u>\n",
    "\n",
    "### Pre-processed Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. Some sample data is show below.\n",
    "\n",
    "<img src='assets/processed_face_data.png' width=60% />\n",
    "\n",
    "> If you are working locally, you can download this data [by clicking here](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be7eb6f_processed-celeba-small/processed-celeba-small.zip)\n",
    "\n",
    "This is a zip file that you'll need to extract in the home directory of this notebook for further loading and processing. After extracting the data, you should be left with a directory of data `processed_celeba_small/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can comment out after executing\n",
    "# !unzip processed_celeba_small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/frankcorrigan/Repos/fake-maps\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/frankcorrigan/Repos/fake-maps'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import problem_unittests as tests\n",
    "# import helper\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the CelebA Data\n",
    "\n",
    "The [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset contains over 200,000 celebrity images with annotations. Since you're going to be generating faces, you won't need the annotations, you'll only need the images. Note that these are color images with [3 color channels (RGB)](https://en.wikipedia.org/wiki/Channel_(digital_image)#RGB_Images) each.\n",
    "\n",
    "### Pre-process and Load the Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. This *pre-processed* dataset is a smaller subset of the very large CelebA data.\n",
    "\n",
    "> There are a few other steps that you'll need to **transform** this data and create a **DataLoader**.\n",
    "\n",
    "#### Exercise: Complete the following `get_dataloader` function, such that it satisfies these requirements:\n",
    "\n",
    "* Your images should be square, Tensor images of size `image_size x image_size` in the x and y dimension.\n",
    "* Your function should return a DataLoader that shuffles and batches these Tensor images.\n",
    "\n",
    "#### ImageFolder\n",
    "\n",
    "To create a dataset given a directory of images, it's recommended that you use PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) wrapper, with a root directory `processed_celeba_small/` and data transformation passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_0.jpg  img_11.jpg img_14.jpg img_17.jpg img_2.jpg  img_5.jpg  img_8.jpg\n",
      "img_1.jpg  img_12.jpg img_15.jpg img_18.jpg img_3.jpg  img_6.jpg  img_9.jpg\n",
      "img_10.jpg img_13.jpg img_16.jpg img_19.jpg img_4.jpg  img_7.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/frankcorrigan/Repos/fake-maps/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>This was a helpful guide: https://www.kaggle.com/ashishpatel26/gan-beginner-tutorial-for-pytorch-celeba-dataset</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Sidenote: How brilliant is HTML5's tag 'mark'? Correct, SO brilliant :) https://stackoverflow.com/questions/25104738/text-highlight-in-markdown</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "def get_dataloader(batch_size, image_size, data_dir='/Users/frankcorrigan/Repos/fake-maps'):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param batch_size: The size of each batch; the number of images in a batch\n",
    "    :param img_size: The square size of the image data (x, y)\n",
    "    :param data_dir: Directory where image data is located\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement function and return a dataloader\n",
    "\n",
    "    # define the appropriate vector of transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Scale(image_size), # resize never seems to work for me even though it's the current best practice\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # define where the data is and what transforms to be applied\n",
    "    dataset = ImageFolder(data_dir, transform)\n",
    "    print(dataset)\n",
    "    \n",
    "    # create the data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                              shuffle=False, num_workers=0, drop_last=False)\n",
    "    \n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataLoader\n",
    "\n",
    "#### Exercise: Create a DataLoader `celeba_train_loader` with appropriate hyperparameters.\n",
    "\n",
    "Call the above function and create a dataloader to view images. \n",
    "* You can decide on any reasonable `batch_size` parameter\n",
    "* Your `image_size` **must be** `32`. Resizing the data to a smaller size will make for faster training, while still creating convincing images of faces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 27\n",
      "    Root location: /Users/frankcorrigan/Repos/fake-maps\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Scale(size=32, interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Define function hyperparameters\n",
    "batch_size = 16 # this was used in MNIST modeling in class\n",
    "img_size = 32 # read above, image size must be 32\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Call your function and get a dataloader\n",
    "images_train_loader = get_dataloader(batch_size, img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 32, 32] at entry 0 and [3, 32, 42] at entry 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4e2f7c557ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages_train_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ahh! remmeber, the size is [batch, channels, height, width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/datasci-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/datasci-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/datasci-env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/datasci-env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/datasci-env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/datasci-env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 32, 32] at entry 0 and [3, 32, 42] at entry 4"
     ]
    }
   ],
   "source": [
    "for data, target in images_train_loader:\n",
    "    print(data.shape)\n",
    "# ahh! remmeber, the size is [batch, channels, height, width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>I continuously get the warning that transforms.Scale is depreciated and I should use Resize instead. However, I can't get Resize to actually work correctly. Scale works well.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can view some images! You should seen square images of somewhat-centered faces.\n",
    "\n",
    "Note: You'll need to convert the Tensor images into a NumPy type and transpose the dimensions to correctly display an image, suggested `imshow` code is below, but it may not be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of images:  16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper display function\n",
    "from PIL import *\n",
    "import PIL.Image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# \"\"\"\n",
    "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "# \"\"\"\n",
    "# # obtain one batch of training images\n",
    "dataiter = iter(images_train_loader)\n",
    "images, _ = dataiter.next() # _ for no labels\n",
    "print(\"Length of images: \", len(images))\n",
    "\n",
    "# # plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "plot_size=6\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Pre-process your image data and scale it to a pixel range of -1 to 1\n",
    "\n",
    "You need to do a bit of pre-processing; you know that the output of a `tanh` activated generator will contain pixel values in a range from -1 to 1, and so, we need to rescale our training images to a range of -1 to 1. (Right now, they are in a range from 0-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the scale function\n",
    "# took guidance from the cycle GAN lesson\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    ''' Scale takes in an image x and returns that image, scaled\n",
    "       with a feature_range of pixel values from -1 to 1. \n",
    "       This function assumes that the input x is already scaled from 0-1.'''\n",
    "    # scale from 0-1 to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# check scaled range\n",
    "# should be close to -1 to 1\n",
    "img = images\n",
    "scaled_img = scale(img)\n",
    "\n",
    "print('Min: ', scaled_img.min())\n",
    "print('Max: ', scaled_img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>The min and max values are close to -1 and 1. Feels like the scaler works.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the Model\n",
    "\n",
    "A GAN is comprised of two adversarial networks, a discriminator and a generator.\n",
    "\n",
    "## Discriminator\n",
    "\n",
    "Your first task will be to define the discriminator. This is a convolutional classifier like you've built before, only without any maxpooling layers. To deal with this complex data, it's suggested you use a deep network with **normalization**. You are also allowed to create any helper functions that may be useful.\n",
    "\n",
    "#### Exercise: Complete the Discriminator class\n",
    "* The inputs to the discriminator are 32x32x3 tensor images\n",
    "* The output should be a single value that will indicate whether a given image is real or fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as recommended in the course videos, creating a helper function to build layers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper conv function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, \n",
    "                           kernel_size, stride, padding, bias=False)\n",
    "    \n",
    "    # append conv layer\n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        # append batchnorm layer\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "     \n",
    "    # using Sequential container\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    # def __init__(self, conv_dim=32):\n",
    "    def __init__(self, conv_dim=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        self.conv_dim = conv_dim\n",
    "\n",
    "        # 32x32 input\n",
    "        self.conv1 = conv(3, conv_dim, 4, batch_norm=False) # first layer, no batch_norm\n",
    "        # 16x16 out\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4) # note that batch_norm is True by default\n",
    "        # 8x8 out\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        # 4x4 out\n",
    "        \n",
    "        # final, fully-connected layer\n",
    "        self.fc = nn.Linear(conv_dim*4*4*4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # all hidden layers + leaky relu activation\n",
    "        out = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.2)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.2)\n",
    "        \n",
    "        # flatten\n",
    "        out = out.view(-1, self.conv_dim*4*4*4)\n",
    "        \n",
    "        # final output layer\n",
    "        out = self.fc(out)        \n",
    "        return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# tests.test_discriminator(Discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The generator should upsample an input and generate a *new* image of the same size as our training data `32x32x3`. This should be mostly transpose convolutional layers with normalization applied to the outputs.\n",
    "\n",
    "#### Exercise: Complete the Generator class\n",
    "* The inputs to the generator are vectors of some length `z_size`\n",
    "* The output should be a image of shape `32x32x3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to Descriminator, using guidance from videos I'm creating a helper function to build layers of model\n",
    "# helper deconv function\n",
    "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a transposed-convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    # create a sequence of transpose + optional batch norm layers\n",
    "    layers = []\n",
    "    transpose_conv_layer = nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                                              kernel_size, stride, padding, bias=False)\n",
    "    # append transpose convolutional layer\n",
    "    layers.append(transpose_conv_layer)\n",
    "    \n",
    "    if batch_norm:\n",
    "        # append batchnorm layer\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    # def __init__(self, z_size, conv_dim=32):\n",
    "    def __init__(self, z_size, conv_dim=32):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        \n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        # first, fully-connected layer\n",
    "        self.fc = nn.Linear(z_size, conv_dim*4*4*4)\n",
    "\n",
    "        # transpose conv layers\n",
    "        self.t_conv1 = deconv(conv_dim*4, conv_dim*2, 4) # note that batch_norm is True by default\n",
    "        self.t_conv2 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.t_conv3 = deconv(conv_dim, 3, 4, batch_norm=False) # last layer, no batch_norm\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # fully-connected + reshape \n",
    "        out = self.fc(x)\n",
    "        out = out.view(-1, self.conv_dim*4, 4, 4) # (batch_size, depth, 4, 4)\n",
    "        \n",
    "        # hidden transpose conv layers + relu\n",
    "        out = F.relu(self.t_conv1(out))\n",
    "        out = F.relu(self.t_conv2(out))\n",
    "        \n",
    "        # last layer + tanh activation\n",
    "        out = self.t_conv3(out)\n",
    "        out = F.tanh(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# tests.test_generator(Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights of your networks\n",
    "\n",
    "To help your models converge, you should initialize the weights of the convolutional and linear layers in your model. From reading the [original DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf), they say:\n",
    "> All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.\n",
    "\n",
    "So, your next task will be to define a weight initialization function that does just this!\n",
    "\n",
    "You can refer back to the lesson on weight initialization or even consult existing model code, such as that from [the `networks.py` file in CycleGAN Github repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py) to help you complete this function.\n",
    "\n",
    "#### Exercise: Complete the weight initialization function\n",
    "\n",
    "* This should initialize only **convolutional** and **linear** layers\n",
    "* Initialize the weights to a normal distribution, centered around 0, with a standard deviation of 0.02.\n",
    "* The bias terms, if they exist, may be left alone or set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    \"\"\"\n",
    "    Applies initial weights to certain layers in a model .\n",
    "    The weights are taken from a normal distribution \n",
    "    with mean = 0, std dev = 0.02.\n",
    "    :param m: A module or layer in a network    \n",
    "    \"\"\"\n",
    "    # classname will be something like:\n",
    "    # `Conv`, `BatchNorm2d`, `Linear`, etc.\n",
    "    # classname = m.__class__.__name__\n",
    "    \n",
    "    # TODO: Apply initial weights to convolutional and linear layers\n",
    "\n",
    "    # I've used the weight init function from the PyTorch documentation\n",
    "    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build complete network\n",
    "\n",
    "Define your models' hyperparameters and instantiate the discriminator and generator from the classes defined above. Make sure you've passed in the correct input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "def build_network(d_conv_dim, g_conv_dim, z_size):\n",
    "    # define discriminator and generator\n",
    "    D = Discriminator(d_conv_dim)\n",
    "    G = Generator(z_size=z_size, conv_dim=g_conv_dim)\n",
    "\n",
    "    # initialize model weights\n",
    "    D.apply(weights_init_normal)\n",
    "    G.apply(weights_init_normal)\n",
    "\n",
    "    print(D)\n",
    "    print()\n",
    "    print(G)\n",
    "    \n",
    "    return D, G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Define model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparams\n",
    "d_conv_dim = 32\n",
    "g_conv_dim = 32\n",
    "z_size = 100 # guidance from course\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "D, G = build_network(d_conv_dim, g_conv_dim, z_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GPU\n",
    "\n",
    "Check if you can train on GPU. Here, we'll set this as a boolean variable `train_on_gpu`. Later, you'll be responsible for making sure that \n",
    ">* Models,\n",
    "* Model inputs, and\n",
    "* Loss function arguments\n",
    "\n",
    "Are moved to GPU, where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Training on GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses for both types of adversarial networks.\n",
    "\n",
    "### Discriminator Losses\n",
    "\n",
    "> * For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_real_loss + d_fake_loss`. \n",
    "* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "\n",
    "### Generator Loss\n",
    "\n",
    "The generator loss will look similar only with flipped labels. The generator's goal is to get the discriminator to *think* its generated images are *real*.\n",
    "\n",
    "#### Exercise: Complete real and fake loss functions\n",
    "\n",
    "**You may choose to use either cross entropy or a least squares error loss to complete the following `real_loss` and `fake_loss` functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    # loss = criterion(D_out.squeeze(), labels)\n",
    "    loss = criterion(D_out.squeeze(1), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    # loss = criterion(D_out.squeeze(), labels)\n",
    "    loss = criterion(D_out.squeeze(1), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "#### Exercise: Define optimizers for your Discriminator (D) and Generator (G)\n",
    "\n",
    "Define optimizers for your models with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# params\n",
    "lr = 0.0002\n",
    "beta1=0.5\n",
    "beta2=0.999 # default value\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
    "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Training will involve alternating between training the discriminator and the generator. You'll use your functions `real_loss` and `fake_loss` to help you calculate the discriminator losses.\n",
    "\n",
    "* You should train the discriminator by alternating on real and fake images\n",
    "* Then the generator, which tries to trick the discriminator and should have an opposing loss function\n",
    "\n",
    "\n",
    "#### Saving Samples\n",
    "\n",
    "You've been given some code to print out some loss statistics and save some generated \"fake\" samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Complete the training function\n",
    "\n",
    "Keep in mind that, if you've moved your models to GPU, you'll also have to move any model inputs to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D, G, n_epochs, print_every=10):\n",
    "    '''Trains adversarial networks for some number of epochs\n",
    "       param, D: the discriminator network\n",
    "       param, G: the generator network\n",
    "       param, n_epochs: number of epochs to train for\n",
    "       param, print_every: when to print and record the models' losses\n",
    "       return: D and G losses'''\n",
    "    \n",
    "    # move models to GPU\n",
    "    if train_on_gpu:\n",
    "        D.cuda()\n",
    "        G.cuda()\n",
    "\n",
    "    # keep track of loss and generated, \"fake\" samples\n",
    "    samples = []\n",
    "    losses = []\n",
    "\n",
    "    # Get some fixed data for sampling. These are images that are held\n",
    "    # constant throughout training, and allow us to inspect the model's performance\n",
    "    sample_size=16\n",
    "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "    fixed_z = torch.from_numpy(fixed_z).float()\n",
    "    # move z to GPU if available\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "\n",
    "    # epoch training loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # batch training loop\n",
    "        for batch_i, (real_images, _) in enumerate(images_train_loader):\n",
    "\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = scale(real_images)\n",
    "\n",
    "            # ===============================================\n",
    "            #         YOUR CODE HERE: TRAIN THE NETWORKS\n",
    "            # ===============================================\n",
    "            \n",
    "            # 1. Train the discriminator on real and fake images\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            # 1. Train with real images\n",
    "\n",
    "            # Compute the discriminator losses on real images \n",
    "            if train_on_gpu:\n",
    "                real_images = real_images.cuda()\n",
    "\n",
    "            D_real = D(real_images)\n",
    "            d_real_loss = real_loss(D_real)\n",
    "\n",
    "            # 2. Train with fake images\n",
    "\n",
    "            # Generate fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            # move x to GPU, if available\n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "            fake_images = G(z)\n",
    "\n",
    "            # Compute the discriminator losses on fake images            \n",
    "            D_fake = D(fake_images)\n",
    "            d_fake_loss = fake_loss(D_fake)\n",
    "\n",
    "            # add up loss and perform backprop\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # 2. Train the generator with an adversarial loss\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            # 1. Train with fake images and flipped labels\n",
    "\n",
    "            # Generate fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "            fake_images = G(z)\n",
    "\n",
    "            # Compute the discriminator losses on fake images \n",
    "            # using flipped labels!\n",
    "            D_fake = D(fake_images)\n",
    "            g_loss = real_loss(D_fake) # use real loss to flip labels\n",
    "\n",
    "            # perform backprop\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            \n",
    "            # ===============================================\n",
    "            #              END OF YOUR CODE\n",
    "            # ===============================================\n",
    "\n",
    "            # Print some loss stats\n",
    "            if batch_i % print_every == 0:\n",
    "                # append discriminator loss and generator loss\n",
    "                losses.append((d_loss.item(), g_loss.item()))\n",
    "                # print discriminator and generator loss\n",
    "                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "\n",
    "        ## AFTER EACH EPOCH##    \n",
    "        # this code assumes your generator is named G, feel free to change the name\n",
    "        # generate and save sample, fake images\n",
    "        G.eval() # for generating samples\n",
    "        samples_z = G(fixed_z)\n",
    "        samples.append(samples_z)\n",
    "        G.train() # back to training mode\n",
    "\n",
    "    # Save training generator samples\n",
    "    with open('train_samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "    \n",
    "    # finally return losses\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your number of training epochs and train your GAN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of epochs \n",
    "n_epochs = 500\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# call training function\n",
    "losses = train(D, G, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n",
    "Plot the training losses for the generator and discriminator, recorded after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator samples from training\n",
    "\n",
    "View samples of images from the generator, and answer a question about the strengths and weaknesses of your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for viewing a list of passed in sample images\n",
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        img = img.detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = ((img + 1)*255 / (2)).astype(np.uint8)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((32,32,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load samples from generator, taken while training\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you notice about your generated samples and how might you improve this model?\n",
    "When you answer this question, consider the following factors:\n",
    "* The dataset is biased; it is made of \"celebrity\" faces that are mostly white\n",
    "* Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "* Optimization strategy; optimizers and number of epochs affect your final result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "<mark><br><br>\n",
    "First, the variety in the faces generated is rather low. Each face is different, but they are mostly all white and in a similar age range. We should consider more data - both in breadth and depth. By this I mean we can expand our dataset to include a more ethnically diverse set of actors & actresses as well as how they are presented (wearing a hat, wearing glasses, etc.). The wild part of this is that when you are creating \"the face of a celebrity\" with AI, the bias in your dataset will be propagated through your audience. Defining what an actor / actress \"looks like\" will result in further standardization of that \"look.\" And you've created a feedback loop where a bias (whether it's good or bad) is now embedded and magnified over time. For me, this removes the most exhilarating component of AI - fueling the human imagination. I believe AI gives us the opportunity to take on new perspectives and find new solutions to old problems. If AI just shows us the same stuff we've seen before, it loses genuine value.<br><br>\n",
    "Second, one of the first -- and perhaps easiest -- ways to improve the model output is to add more layers. This will allow the model to learn more features and produce more life-like faces.<br><br> \n",
    "Third, I trained the model for 10 epochs. Although I am pleasantly surprised how impressive the results are... there is room for improvement. The images are still a bit blurry and in some images there seem to be black spaces in parts of the image. I see a drastic difference between training for 1 epoch and 10 epcohs. I am confident that more epochs would improve the results.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_face_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
